# LLM Local com LLaMA 2 e FastAPI

Bem-vindo ao **LLM Local com LLaMA 2 e FastAPI**, uma aplicaÃ§Ã£o web moderna e eficiente para gerar resumos automÃ¡ticos de textos usando inteligÃªncia artificial. Com uma interface intuitiva e processamento rÃ¡pido, vocÃª pode transformar qualquer texto em um resumo conciso e baixar o resultado como um arquivo Word.

## ğŸš€ Recursos Principais

- ğŸŒ **Interface Web Intuitiva**: Envie textos facilmente e visualize os resumos gerados.
- ğŸ§  **Resumos Inteligentes com LLaMA 2**: Utiliza o poderoso modelo LLaMA 2 para criar resumos precisos e eficientes.
- ğŸ“„ **ExportaÃ§Ã£o para Word**: Salve seus resumos em arquivos `.docx` para fÃ¡cil compartilhamento e ediÃ§Ã£o.
- ğŸ“¥ **Download Direto**: Baixe os resumos rapidamente pela interface web.

---

## ğŸ“Œ Requisitos

Antes de executar a aplicaÃ§Ã£o, certifique-se de ter os seguintes itens instalados:

- **Python 3.8+**
- **FastAPI**
- **Jinja2**
- **Python-docx**
- **Ollama** (para rodar o modelo LLaMA 2)

---
Caso nÃ£o tenha instalado baixe e instale o Ollama a partir do link oficial:
https://ollama.com/download

## âš™ï¸ InstalaÃ§Ã£o

1ï¸âƒ£ Clone este repositÃ³rio:
```sh
 git clone https://github.com/MatheusMV05/LLM-Local-API-Project.git
```
2ï¸âƒ£ Crie e ative um ambiente virtual:
```sh
python -m venv env
source env/bin/activate  # No Windows: env\Scripts\activate
```
3ï¸âƒ£ Instale as dependÃªncias:
```sh
pip install -r requirements.txt
```
---

## â–¶ï¸ Como Usar

1ï¸âƒ£ Inicie o servidor FastAPI:
```sh
uvicorn main:app --reload
```
2ï¸âƒ£ Acesse a aplicaÃ§Ã£o no navegador:
```sh
http://127.0.0.1:8000
```
3ï¸âƒ£ Insira o texto no campo apropriado e clique em **Resumir**.

4ï¸âƒ£ Visualize o resumo gerado e baixe-o como um arquivo Word.

ğŸ’¡ **Dica:** Experimente textos de diferentes tamanhos e veja como o LLaMA 2 se adapta para criar resumos otimizados!

---

ğŸš€ **Experimente agora e aproveite a praticidade dos resumos automÃ¡ticos com LLaMA 2!**

